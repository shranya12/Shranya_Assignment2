{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shranya12/Shranya_Assignment2/blob/main/Shranya_603_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCdivaZ0FN-X",
        "outputId": "3617511c-b0cb-4e2f-c716-08f7ef24a1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: 101\n",
            "of: 42\n",
            "and: 39\n",
            "to: 38\n",
            "a: 38\n",
            "you: 38\n",
            "i: 37\n",
            "his: 21\n",
            "voice: 20\n",
            "wormtail: 18\n",
            "â€”: 17\n",
            "was: 16\n",
            "is: 16\n",
            "said: 16\n",
            "he: 15\n",
            "it: 15\n",
            "my: 15\n",
            "will: 15\n",
            "me: 14\n",
            "be: 14\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Defineing the file path\n",
        "file_path = \"/content/File 1.txt\"\n",
        "\n",
        "def map_reduce_word_count(file_path):\n",
        "    # Reading thee file\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read().lower()\n",
        "\n",
        "    # removeing punctuation\n",
        "    words = [word.strip(\".,!?;:\\\"'()[]\") for word in text.split()]\n",
        "\n",
        "    # Using Map Created a key-value pair\n",
        "    word_map = defaultdict(int)\n",
        "    for word in words:\n",
        "        word_map[word] += 1  # Increment count\n",
        "\n",
        "    # Using reduce combineing results\n",
        "    return word_map\n",
        "\n",
        "\n",
        "word_counts = map_reduce_word_count(file_path)\n",
        "\n",
        "# Displaying top 20 most common words from file\n",
        "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, count in sorted_words[:20]:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "#My date of birth is 07/july/2001\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import words\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('words')\n",
        "\n",
        "def is_non_english(word, english_words):\n",
        "    return word.lower() not in english_words and word.istitle()\n",
        "\n",
        "def map_reduce_non_english_words(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    english_words = set(words.words())  # Loading the words\n",
        "    potential_words = re.findall(r'\\b[A-Z][a-z]+\\b', text)  # Finding capitalized words\n",
        "    non_english_words = [word for word in potential_words if is_non_english(word, english_words)]\n",
        "\n",
        "    word_counts = Counter(non_english_words)\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "# Specifying path\n",
        "file_path = \"/content/file 2.txt\"\n",
        "\n",
        "# Running and printing thew results\n",
        "word_counts = map_reduce_non_english_words(file_path)\n",
        "for word, count in word_counts.items():\n",
        "    print(f'{word}: {count}')\n",
        "\n",
        "#My date of birth is 07/july/2001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoPwcO5jHEux",
        "outputId": "135c7400-4df2-448b-a508-65123e0f06af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hermione: 6\n",
            "Ron: 8\n",
            "Mrs: 9\n",
            "Weasley: 11\n",
            "Charlie: 9\n",
            "Knives: 1\n",
            "Fred: 4\n",
            "George: 3\n",
            "Rowling: 5\n",
            "Hogwarts: 2\n",
            "Crookshanks: 2\n",
            "Percy: 8\n",
            "Perce: 1\n",
            "Chuckling: 1\n",
            "Weasleys: 1\n",
            "Mr: 7\n",
            "Tuesday: 1\n",
            "Games: 1\n",
            "Bertha: 3\n",
            "Jorkins: 1\n",
            "Albania: 2\n",
            "Australia: 1\n",
            "Cooperation: 1\n",
            "Ireland: 2\n",
            "Peru: 1\n",
            "Bulgaria: 1\n",
            "Viktor: 1\n",
            "Krum: 2\n",
            "England: 1\n",
            "Transylvania: 1\n",
            "Wales: 1\n",
            "Uganda: 1\n",
            "Scotland: 1\n",
            "Luxembourg: 1\n",
            "Gryffindor: 1\n",
            "Quidditch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}